{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knotworking: An Inverse-Problem Approach to Phase I Drug Discovery\n",
    "\n",
    "## Project Description\n",
    "\n",
    "This project implements a **10-layer end-to-end computational drug discovery pipeline** that reframes early-stage drug discovery as an *inverse problem*: rather than screening existing compound libraries, we start from a patient's genomic and clinical profile and work *backwards* to generate and validate novel candidate molecules that are mathematically proven to satisfy drug-likeness constraints.\n",
    "\n",
    "The pipeline is novel in three ways:\n",
    "\n",
    "1. **Patient-first reasoning** — Layers 1–6 translate raw patient data (symptoms, genetic variants, lab values) into a ranked, high-confidence drug target before any chemistry is done.\n",
    "2. **Generative molecular design** — Layer 7 uses a Bayesian Graph Variational Autoencoder (BayesianGraphVAE) trained on 250 k ZINC drug-like molecules to *generate* candidate structures conditioned on the identified target, guided by a feedback loop from the validation layer.\n",
    "3. **Formal verification** — Layer 9 uses the Rocq (Coq) proof assistant together with an LLM (Claude) to produce machine-checkable proofs that each candidate molecule satisfies Lipinski's Rule of Five, Veber rules, and structural safety constraints — providing regulatory-grade auditability that black-box ML alone cannot.\n",
    "\n",
    "### Key Technologies\n",
    "| Component | Technology |\n",
    "|-----------|------------|\n",
    "| Patient genomics | Ensembl VEP, gnomAD, ClinVar APIs |\n",
    "| Protein interaction graph | STRING DB, Reactome |\n",
    "| Target scoring | Open Targets Platform, ChEMBL |\n",
    "| Molecular generation | BayesianGraphVAE (PyTorch) |\n",
    "| Property prediction | MC-Dropout MLP — 6 outputs |\n",
    "| Formal verification | Rocq/Coq (14 chemistry axiom files) |\n",
    "| LLM proof generation | Claude 3.5 Haiku (Anthropic) |\n",
    "| Cheminformatics | RDKit |\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Completeness Review\n",
    "\n",
    "The README specifies a 10-layer pipeline. The table below maps each layer to its implementation status.\n",
    "\n",
    "| Layer | README Description | Implementation File(s) | Status |\n",
    "|-------|--------------------|------------------------|--------|\n",
    "| 1 | Data Ingestion (patient data → standardised format) | `src/data/data_loader.py` (ZINC CSV only) | ⚠ Partial — clinical/imaging ingestion missing |\n",
    "| 2 | Disease Prediction ML (diagnose subtype/severity) | `src/pipeline/layer2_patient_intake.py` | ⚠ Partial — schema + validation done; ML classifier not implemented |\n",
    "| 3 | Genotype-to-Phenotype (GWAS, variant effect) | `src/pipeline/layer3_genotype_phenotype.py` | ✅ Complete |\n",
    "| 4 | Causal Pathway Modeling (protein interaction graph) | `src/pipeline/layer4_causal_pathway.py` | ✅ Complete |\n",
    "| 5 | Target Identification (druggable proteins) | `src/pipeline/layer5_target_identification.py` | ✅ Complete |\n",
    "| 6 | Target Confidence Scoring (multi-signal aggregation) | `src/pipeline/layer6_target_confidence.py` | ✅ Complete |\n",
    "| 7 | Molecular Generation (VAE/diffusion/RL) | `model.py`, `run_pipeline.py` | ⚠ Partial — VAE architecture done; generation uses hardcoded SMILES pool |\n",
    "| 8 | Property Prediction (binding, solubility, toxicity) | `src/ml/property_predictor.py` | ✅ Complete |\n",
    "| 9 | Computational Drug Validation (RDKit + formal proof) | `src/llm/pipeline_layer9.py`, `src/rocq/*.v` | ✅ Complete |\n",
    "| 10 | Multi-Fidelity Screening (ML → MD → QM → experiments) | — | ❌ Not implemented |\n",
    "\n",
    "**Feedback loop** (Layer 9 → Layer 7): ✅ Implemented in `src/llm/feedback_controller.py`\n",
    "\n",
    "### Gap Summary\n",
    "- **Layer 1**: No parser for medical imaging or EHR documents; only tabular CSV is handled.\n",
    "- **Layer 2**: The README specifies an ML disease classifier; the current code only provides a structured data schema (`PatientRecord`) and input validation — no trained model.\n",
    "- **Layer 7**: `BayesianGraphVAE` is fully defined and trains correctly, but `run_pipeline.py` samples from a hardcoded SMILES pool instead of decoding from the latent space of a trained model.\n",
    "- **Layer 10**: Placeholder only — no molecular dynamics, quantum chemistry, or experimental integration exists.\n",
    "\n",
    "**Overall completeness: ~70%** — all critical validation and target-selection logic is production-quality; the main gaps are generative model deployment (L7) and multi-fidelity screening (L10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = os.path.abspath('.')\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Scientific stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Cheminformatics\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors, QED, AllChem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "print('Python     :', sys.version.split()[0])\n",
    "print('PyTorch    :', torch.__version__)\n",
    "print('NumPy      :', np.__version__)\n",
    "print('Pandas     :', pd.__version__)\n",
    "print('CUDA avail :', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 1 — Data Ingestion\n",
    "\n",
    "Load and inspect the ZINC 250k drug-like molecule dataset.  \n",
    "**TODO**: extend to accept clinical JSON / HL7 FHIR patient records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.data_loader import prepare_dataset\n",
    "\n",
    "# Prepare dataset: fingerprints, labels, toxic flags\n",
    "# Returns tensors written to disk; loads them here for inspection\n",
    "X, y, toxic = prepare_dataset()\n",
    "\n",
    "print(f'Fingerprint matrix : {X.shape}   (N_molecules x 2048 bits)')\n",
    "print(f'QED labels         : {y.shape}')\n",
    "print(f'Toxic flags        : {toxic.shape}  ({toxic.sum().item()} flagged)')\n",
    "print(f'Mean QED           : {y.mean():.3f}  (1.0 = perfectly drug-like)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check — inspect a few molecules\n",
    "df = pd.read_csv('src/data/250k_rndm_zinc_drugs_clean_3.csv').sample(5, random_state=0)\n",
    "df[['smiles', 'qed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 2 — Patient Intake & Disease Prediction\n",
    "\n",
    "`PatientRecord` captures demographics, clinical data, and genetic variants.  \n",
    "**TODO**: train an ML classifier (e.g. gradient-boosted tree or transformer) on top of this schema to predict disease subtype/severity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.layer2_patient_intake import PatientRecord, GeneVariant, LabValue\n",
    "\n",
    "# Example patient record\n",
    "patient = PatientRecord(\n",
    "    patient_id='PT-0001',\n",
    "    age=52,\n",
    "    sex='M',\n",
    "    symptoms=['fatigue', 'weight loss', 'night sweats'],\n",
    "    diagnosis='Non-Hodgkin Lymphoma',\n",
    "    disease_subtype='DLBCL',\n",
    "    severity='stage-III',\n",
    "    genes_of_interest=['MYC', 'BCL2', 'TP53'],\n",
    "    gene_variants=[\n",
    "        GeneVariant(gene='MYC', hgvs='c.202G>A', variant_type='missense', zygosity='heterozygous'),\n",
    "        GeneVariant(gene='TP53', hgvs='c.817C>T', variant_type='nonsense', zygosity='homozygous'),\n",
    "    ],\n",
    "    lab_values=[\n",
    "        LabValue(name='LDH', value=420.0, unit='U/L', reference_range=(120, 240)),\n",
    "        LabValue(name='WBC', value=14.2,  unit='×10⁹/L', reference_range=(4.0, 11.0)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "patient.validate()\n",
    "print('HGVS notations:', patient.hgvs_notations())\n",
    "print('Serialised record:')\n",
    "print(json.dumps(patient.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 3 — Genotype-to-Phenotype Association\n",
    "\n",
    "Calls Ensembl VEP, gnomAD, and ClinVar to annotate each variant and identify dysfunctional proteins.  \n",
    "> **Note**: requires internet access and may take ~10–30 s per variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.layer3_genotype_phenotype import GenotypePhenotypeMapper\n",
    "\n",
    "mapper = GenotypePhenotypeMapper()\n",
    "gp_result = mapper.run(patient)\n",
    "\n",
    "print('Dysfunctional proteins identified:')\n",
    "for protein in gp_result.dysfunctional_proteins:\n",
    "    print(f'  • {protein}')\n",
    "\n",
    "print('\\nVariant consequences:')\n",
    "for vc in gp_result.variant_consequences:\n",
    "    print(f'  {vc.hgvs}: impact={vc.impact}, consequence={vc.consequence}')\n",
    "\n",
    "print('\\nGene constraint scores (pLI > 0.9 → not safe to target):')\n",
    "for gc in gp_result.gene_constraints:\n",
    "    print(f'  {gc.gene}: pLI={gc.pli:.2f}, LOEUF={gc.loeuf:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 4 — Causal Pathway Modeling\n",
    "\n",
    "Builds a directed protein–protein interaction graph using STRING DB and Reactome, then ranks nodes by betweenness centrality (bottleneck = best target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.layer4_causal_pathway import CausalPathwayModeler\n",
    "\n",
    "modeler = CausalPathwayModeler()\n",
    "pathway_result = modeler.run(gp_result.dysfunctional_proteins)\n",
    "\n",
    "print('Top pathway candidates (by betweenness centrality):')\n",
    "for i, candidate in enumerate(pathway_result.top_candidates[:5], 1):\n",
    "    print(f'  {i}. {candidate.protein}  centrality={candidate.centrality:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 5 — Target Identification\n",
    "\n",
    "Queries the Open Targets Platform and ChEMBL to score each candidate protein on druggability, tractability, and known drug interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.layer5_target_identification import TargetIdentifier\n",
    "\n",
    "identifier = TargetIdentifier()\n",
    "target_result = identifier.run(pathway_result.top_candidates, patient)\n",
    "\n",
    "print(f'Chosen target: {target_result.chosen_target.gene}')\n",
    "print(f'  Open Targets association score : {target_result.chosen_target.ot_score:.3f}')\n",
    "print(f'  ChEMBL known drugs             : {target_result.chosen_target.chembl_drug_count}')\n",
    "print(f'  Tractability (small molecule)  : {target_result.chosen_target.tractability_sm}')\n",
    "print(f'  Binding pocket exists          : {target_result.chosen_target.has_binding_pocket}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 6 — Target Confidence Scoring\n",
    "\n",
    "Combines multi-signal evidence into a single deterministic confidence score [0, 1] for regulatory auditability. Essential genes and safety-flagged targets are penalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline.layer6_target_confidence import TargetConfidenceScorer\n",
    "\n",
    "scorer = TargetConfidenceScorer()\n",
    "confidence = scorer.score(\n",
    "    target_result.chosen_target,\n",
    "    pathway_result,\n",
    "    gp_result,\n",
    ")\n",
    "\n",
    "print(f'Target confidence score: {confidence.final_score:.3f}')\n",
    "print(f'  OT contribution       : {confidence.ot_contribution:.3f}')\n",
    "print(f'  Centrality bonus      : {confidence.centrality_contribution:.3f}')\n",
    "print(f'  Essential gene penalty: {confidence.essential_penalty:.3f}')\n",
    "print(f'  Safety flag penalty   : {confidence.safety_penalty:.3f}')\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "if confidence.final_score >= CONFIDENCE_THRESHOLD:\n",
    "    print(f'\\n✅ Target cleared for molecular generation (score ≥ {CONFIDENCE_THRESHOLD})')\n",
    "else:\n",
    "    print(f'\\n❌ Target below threshold — revisit Layer 5 candidates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 7 — Molecular Generation (BayesianGraphVAE)\n",
    "\n",
    "Trains (or loads) the Bayesian Graph VAE on ZINC fingerprints, then decodes candidate molecules from the latent space.  \n",
    "**TODO**: condition the latent-space sampling on the target protein embedding to bias generation towards binders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BayesianGraphVAE\n",
    "\n",
    "FINGERPRINT_DIM = 2048\n",
    "LATENT_DIM      = 16\n",
    "MODEL_PATH      = 'models/model.pt'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vae    = BayesianGraphVAE(input_dim=FINGERPRINT_DIM, latent_dim=LATENT_DIM).to(device)\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    vae.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    print(f'Loaded trained VAE from {MODEL_PATH}')\n",
    "else:\n",
    "    print('No saved model found — training from scratch (300 epochs).')\n",
    "    print('Run: python model.py   to train and save.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training loop (skip if model already saved) ---\n",
    "TRAIN_VAE = not os.path.exists(MODEL_PATH)   # set True to force re-training\n",
    "\n",
    "if TRAIN_VAE:\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    X_train = X.to(device)\n",
    "    dataset = TensorDataset(X_train)\n",
    "    loader  = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "    EPOCHS    = 300\n",
    "\n",
    "    vae.train()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        total_loss = 0.0\n",
    "        for (batch,) in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss, *_ = vae(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'Epoch {epoch:3d}/{EPOCHS}  loss={total_loss / len(loader):.4f}')\n",
    "\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    torch.save(vae.state_dict(), MODEL_PATH)\n",
    "    print('Model saved to', MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate candidate molecules by sampling the latent space\n",
    "vae.eval()\n",
    "N_CANDIDATES = 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    z         = torch.randn(N_CANDIDATES, LATENT_DIM, device=device)\n",
    "    fp_recon  = vae.decode(z).cpu().numpy()        # (N, 2048) soft fingerprints\n",
    "    fp_binary = (fp_recon > 0.5).astype(int)\n",
    "\n",
    "print(f'Generated {N_CANDIDATES} candidate fingerprints.')\n",
    "print('Mean bit density:', fp_binary.mean())\n",
    "\n",
    "# NOTE: Converting binary fingerprints back to valid SMILES requires\n",
    "# a fingerprint-inversion model or nearest-neighbour lookup in the training set.\n",
    "# TODO: integrate REINVENT / graph-based decoder for valid SMILES output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 8 — Property Prediction (MC-Dropout MLP)\n",
    "\n",
    "Predicts 6 physicochemical properties with uncertainty estimates via Monte Carlo Dropout (50 forward passes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.property_predictor import PropertyPredictor\n",
    "\n",
    "PROPERTY_NAMES = ['QED', 'LogP', 'MW', 'TPSA', 'HBD', 'HBA']\n",
    "predictor = PropertyPredictor(input_dim=FINGERPRINT_DIM, n_outputs=len(PROPERTY_NAMES))\n",
    "\n",
    "# Predict on the first generated fingerprint using MC-Dropout\n",
    "sample_fp = torch.tensor(fp_binary[:1], dtype=torch.float32)\n",
    "means, variances = predictor.predict_with_uncertainty(sample_fp, n_samples=50)\n",
    "\n",
    "print('Property predictions for candidate #0:')\n",
    "print(f'{\"Property\":<8} {\"Mean\":>10} {\"Std Dev\":>10}')\n",
    "print('-' * 32)\n",
    "for name, mu, var in zip(PROPERTY_NAMES, means[0], variances[0]):\n",
    "    print(f'{name:<8} {mu:>10.3f} {var**0.5:>10.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 9 — Computational Drug Validation (Rocq + LLM)\n",
    "\n",
    "Four-checkpoint validation pipeline:\n",
    "1. **Lipinski Rule of Five** — MW ≤ 500, LogP ≤ 5, HBD ≤ 5, HBA ≤ 10\n",
    "2. **Veber Rules** — RotBonds ≤ 10, PSA ≤ 140\n",
    "3. **Safety Filters** — PAINS & Brenk structural alerts (via RDKit)\n",
    "4. **Formal Rocq Proof** — LLM-generated, machine-verified by `coqc`\n",
    "\n",
    "> **Requires**: Anthropic API key set as `ANTHROPIC_API_KEY` in the environment, and `coqc` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm.pipeline_layer9 import DrugValidator\n",
    "\n",
    "# Use an example SMILES (aspirin) for demonstration\n",
    "DEMO_SMILES = 'CC(=O)Oc1ccccc1C(=O)O'   # Aspirin\n",
    "\n",
    "validator = DrugValidator()\n",
    "result    = validator.validate(DEMO_SMILES)\n",
    "\n",
    "print(f'SMILES           : {DEMO_SMILES}')\n",
    "print(f'Confidence score : {result.confidence_score:.2f}  ({result.confidence_score*100:.0f}% checks passed)')\n",
    "print()\n",
    "for check_name, passed in result.checks.items():\n",
    "    icon = '✅' if passed else '❌'\n",
    "    print(f'  {icon}  {check_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the generated Rocq proof (if coqc is available)\n",
    "if result.rocq_proof:\n",
    "    print('Generated Rocq proof:')\n",
    "    print('─' * 60)\n",
    "    print(result.rocq_proof)\n",
    "    print('─' * 60)\n",
    "    print('Verified by coqc:', result.rocq_verified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feedback Loop — Layer 9 → Layer 7\n",
    "\n",
    "When validation fails, `FeedbackController` tightens the VAE sampling constraints and re-generates candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm.feedback_controller import FeedbackController\n",
    "\n",
    "controller = FeedbackController()\n",
    "constraints = controller.extract_constraints(result)\n",
    "\n",
    "print('Updated sampling constraints after failed validation:')\n",
    "for k, v in constraints.items():\n",
    "    print(f'  {k}: {v}')\n",
    "\n",
    "# Apply soft constraint to the VAE reparameterize step\n",
    "controller.update_vae_sampling(vae, constraints)\n",
    "print('\\nVAE sampling updated — re-run Layer 7 to generate constrained candidates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Layer 10 — Multi-Fidelity Screening\n",
    "\n",
    "> **Status: Not yet implemented.**\n",
    "\n",
    "Planned hierarchical filtering pipeline:\n",
    "\n",
    "```\n",
    "Fast ML score  →  Molecular Dynamics  →  Quantum Chemistry  →  Experimental assay\n",
    "  (~1 ms/mol)       (~1 min/mol)           (~1 hr/mol)          (~1 day/mol)\n",
    "```\n",
    "\n",
    "Each stage eliminates ~90% of candidates, so only the top ~0.001% of generated molecules reach wet-lab testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement multi-fidelity screening\n",
    "#\n",
    "# Suggested libraries:\n",
    "#   - OpenMM / GROMACS bindings  → molecular dynamics\n",
    "#   - PySCF / Psi4               → quantum chemistry (DFT)\n",
    "#   - AutoDock-GPU               → docking scores as proxy for binding affinity\n",
    "\n",
    "def multifidelity_screen(smiles_list):\n",
    "    \"\"\"Placeholder — returns all candidates unchanged.\"\"\"\n",
    "    raise NotImplementedError('Layer 10 is not yet implemented.')\n",
    "\n",
    "print('Layer 10 placeholder registered.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End-to-End Pipeline Demo\n",
    "\n",
    "Runs the full pipeline on an example patient and a small pool of candidate molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_SMILES = [\n",
    "    'CC(=O)Oc1ccccc1C(=O)O',  # Aspirin\n",
    "    'CC12CCC3C(C1CCC2O)CCC4=CC(=O)CCC34C',  # Testosterone\n",
    "    'CN1CCC[C@H]1c2cccnc2',    # Nicotine\n",
    "    'C1=CC=C2C(=C1)C=CC=C2',   # Azulene\n",
    "    'c1ccc2c(c1)cc1ccc3cccc4ccc2c1c34',  # Pyrene (PAINS alert expected)\n",
    "]\n",
    "\n",
    "print(f'Validating {len(CANDIDATE_SMILES)} candidate molecules...\\n')\n",
    "validator = DrugValidator()\n",
    "\n",
    "results = []\n",
    "for smi in CANDIDATE_SMILES:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    name = mol.GetPropsAsDict().get('_Name', smi[:20]) if mol else smi[:20]\n",
    "    try:\n",
    "        res = validator.validate(smi)\n",
    "        results.append((smi, res.confidence_score, res.checks))\n",
    "        status = '✅ PASS' if res.confidence_score >= 0.75 else '⚠  LOW' if res.confidence_score >= 0.5 else '❌ FAIL'\n",
    "        print(f'{status}  score={res.confidence_score:.2f}  {smi[:35]}')\n",
    "    except Exception as e:\n",
    "        print(f'ERROR  {smi[:35]}  ({e})')\n",
    "\n",
    "# Rank survivors\n",
    "survivors = [(smi, score) for smi, score, _ in results if score >= 0.75]\n",
    "survivors.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(f'\\n{len(survivors)}/{len(CANDIDATE_SMILES)} candidates cleared for Layer 10 screening.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results & Discussion\n",
    "\n",
    "### What worked well\n",
    "- The rule-based scoring in Layers 3–6 is fully deterministic and auditable — every target confidence score can be traced back to a specific API response or formula weight.\n",
    "- The Rocq formal verification library covers all 118 elements, 12+ drug-likeness filters, and produces machine-checkable proofs — a genuine novelty over existing pipelines.\n",
    "- The MC-Dropout property predictor gives calibrated uncertainty estimates, flagging low-confidence predictions before they propagate to expensive downstream steps.\n",
    "\n",
    "### Current limitations & next steps\n",
    "\n",
    "| Gap | Suggested fix |\n",
    "|-----|---------------|\n",
    "| Layer 2 — no ML disease classifier | Fine-tune a small transformer or GBT on TCGA/GEO labelled expression data |\n",
    "| Layer 7 — hardcoded SMILES pool | Deploy trained VAE decoder; or integrate REINVENT 4 / DiffSBDD for structure-based generation |\n",
    "| Layer 10 — no docking | Wrap AutoDock-GPU or Vina as a subprocess; feed docking score as an additional reward signal to L7 |\n",
    "| Fingerprint inversion | Train a separate graph decoder (CGVAE / JTVAE) that maps latent vectors to valid molecular graphs |\n",
    "| LLM proof reliability | Fine-tune a small code model on `rocq_proof_corpus.jsonl` to reduce retry count |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "if results:\n",
    "    scores = [s for _, s, _ in results]\n",
    "    print(f'Candidates evaluated : {len(scores)}')\n",
    "    print(f'Mean confidence score: {np.mean(scores):.3f}')\n",
    "    print(f'Max confidence score : {max(scores):.3f}')\n",
    "    print(f'Passing (≥ 0.75)     : {sum(1 for s in scores if s >= 0.75)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
